{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQIKTGGMF5n5"
      },
      "source": [
        "# Multiprocessing\n",
        "\n",
        "Or writing programs to use multiple processor cores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a41G39GsF5n8"
      },
      "source": [
        "### This lesson is based on notes from [sebastianraschka.com](http://sebastianraschka.com/Articles/2014_multiprocessing.html)\n",
        "\n",
        "What actually happens when you run code, or execute a program on a computer? If we focus on Python, what happens when we run Python code? Does our computer \"speak Python\", understand what we type, and do what we ask?\n",
        "\n",
        "It turns out that no, our computer does NOT speak Python. When we execute Python code, our code is passed to an **interpreter**. This interpreter is what reads our code and translates what we have written into what is called **machine code**. Machine code is typically not human-readable. That's the reason we have languages like Python, R, C++, Java, etc. We need a way to express logic clearly and explicitly, so that it can be translated into the kind of code that our computer DOES understand.\n",
        "\n",
        "So far, human speech (or written human language) does not translate directly to machine code.\n",
        "\n",
        "But what actually happens once we write our code, and the computer starts to run that code? We can understand the basics of this process through the following six (simplified!) steps:\n",
        "\n",
        "1) The computer allocates memory to the program\n",
        "2) The program issues a series of instructions to the processor (the thinking part of the computer)\n",
        "3) Upon completion of one instruction by the processor, the next is started\n",
        "4) Information is returned from the processor to the program as needed\n",
        "5) New instructions are sent to the processor based on information received from the processor\n",
        "6) Return to step 2, repeat until program is finished\n",
        "\n",
        "\n",
        "When a program is running, it typically has a single space in memory in which it stores all information relevant to its task. This memory space allows the information to be used by whichever part of the program requires that information to use it. Basically, think of all of the things in memory as shared assets, and different parts of the program want to be able to access them at the same time. Kind of like if you share a storage space with a friend or family member. Sometimes you'll be pulling out the camping gear, and sometimes they will be instead.\n",
        "\n",
        "Information that is accessible across the entire program are called **globally defined values**. But not all variables are global. Some variables with reduced **scope** are not available to all segments of a program. You might have valuables that you don't share with the other people using your storage space. It's kind of like that. Variables with reduced scope are only available to parts of the program that share that scope.\n",
        "\n",
        "While for people, we restrict access primarily based on keeping valuables safe, programs tend to do it in order to ensure that information is not lost or modified while it is needed by a part of the program. You wouldn't want to have someone change your budgeting spreadsheet in the middle of the month, would you?\n",
        "\n",
        "Again, **scope** is a term used to define the areas in which a given value in memory is accessible. Variables that are **Global** in scope can typically be accessed by any function or command running as part of the program. Variables that are **Local** can only be accessed within the scope in which they are defined. A variable created inside a function is said to be **local** to that function, and only available until the function is concluded. Then it is either returned to the global space, or forgotten.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpoAG4l4F5n9"
      },
      "source": [
        "\n",
        "![](https://github.com/dustywhite7/Econ8320/blob/master/SlidesCode/serialParallel.png?raw=true)\n",
        "\n",
        "One of the single biggest changes in computational technology in the recent past was the advent and spread of parallel computation. This concept is the crux of this lesson, but needs a lot of explaining, because it isn't something that we as computer USERS have to deal with. It happens behind the scenes, despite its tremendous importance.\n",
        "\n",
        "### Sequential programs\n",
        "\n",
        "When we perform tasks, some steps must be performed sequentially. This happens because one task depends on the results of the other task. For example, an American football team will only make a play on fourth down if they fail to score or convert on third down. We can't fully implement our plan for fourth down until we have seen whether or not fourth down will occur.\n",
        "\n",
        "We need to aim our penalty shot in FIFA before taking the kick. We even need to wait to complete the details for the elimination rounds of the tournament until group play has ended. Until then, we don't know enough about which teams will advance to complete the bracket.\n",
        "\n",
        "In programming speak, sequential programs are sequential because it is critical that the results of one calculation be within the **scope** of the other calculations. If one sequential calculation cannot view the results of the prior calculation, then the second function cannot be completed.\n",
        "\n",
        "### Parallel Programs\n",
        "\n",
        "On the other hand, some calculations can be performed independent of the results of other steps. These tend to be calculations that can essentially be considered separate tasks, but part of a larger overall task. Some examples include\n",
        "\n",
        "- Batch processing of files\n",
        "- Non-sequential simulations\n",
        "- Serving recommended products to many users\n",
        "- Repeated random draws\n",
        "- Rendering polygons\n",
        "\n",
        "The key difference between serial and parallel programs is the dependency (or lack of dependency) of calculations on the results of previous calculations.\n",
        "\n",
        "- Serial programs rely on previous results\n",
        "- Parallel programs do not depend on the results of other calculations\n",
        "\n",
        "Parallel programs can occur simultaneously, allowing us to accelerate execution. If we can run four calculations simultaneously, then in theory we might be able to run 100 computations in the time that a serial program might run 25 similar computations.\n",
        "\n",
        "Parallel computations have enabled advances like improved graphics in video games, and have made possible the revolution in machine learning. Let's learn how to make parallel code work for our projects, as well!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UC_zjAICF5n9"
      },
      "source": [
        "One place where parallel processing really shines is in estimating complex mathematical calculations. While some math can be solved algebraically, other math problems can only be solved computationally. The more computations we can perform in a given amount of time, the sooner we can find the solution to those problems. We can buy faster computers to speed up calculations, but we can also parallelize many calculations, giving us benefits without having to upgrade our computer!\n",
        "\n",
        "Some examples in which we will typically solve math problems with computation:\n",
        "- Estimating Producer/Consumer Surplus\n",
        "- Calculating probabilities from frequency tables\n",
        "- Integrating on complicated functional forms\n",
        "\n",
        "### Numeric Integration\n",
        "\n",
        "Often, when integrating complicated functions, there is no **algebraic** solution to the integral. This means that we need to estimate the value of the integral **numerically**. The process that we follow to estimate an integral numerically follows:\n",
        "\n",
        "1) Choose points at which to estimate the value of the function\n",
        "2) Choose bandwidth\n",
        "3) Multiply function values by bandwidth\n",
        "4) Add all estimates to calculate approximate integral\n",
        "\n",
        "![](https://github.com/dustywhite7/Econ8320/blob/master/SlidesCode/integralProcess.png?raw=true)\n",
        "\n",
        "Let's start by just defining a function to integrate. We can use\n",
        "\n",
        "$$ f(x) = \\frac{1}{1+x^2} $$\n",
        "\n",
        "as our example function. First, we need to write this function as a Python function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZye1RtlF5n-"
      },
      "outputs": [],
      "source": [
        "# define any function here!\n",
        "def f(x):\n",
        "    # return the value of the function given x\n",
        "    return 1/(1 + x**2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_fuectyF5n-"
      },
      "source": [
        "Now, we need to write a function that can sample from our function, and calculate the area of the sampled rectangles, and then return an approximate area."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQpETcakF5n_"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def serial_integral(nSample, f, xmin, xmax):\n",
        "  # determine points of estimation\n",
        "  sample = sorted([random.uniform(xmin, xmax) for i in range(nSample)])\n",
        "  # Calculate height at each point\n",
        "  value = [f(i) for i in sample]\n",
        "  # Calculate areas of rectangles\n",
        "  # We have to specially calculate the first rectangle,\n",
        "  #   because xmin is not part of our list of samples\n",
        "  area = [(sample[0]-xmin)*value[0]] +\n",
        "    [(sample[i]-sample[i-1])*value[i] for i in range(1, len(sample))]\n",
        "  # Return sum as an approximate integral\n",
        "  return sum(area)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dv8u3oxHF5n_"
      },
      "source": [
        "This is our function for actually integrating a function `f` from `xmin` to `xmax` across `nSample` random intervals. Below is a picture of how this function gets (approximately) closer to estimating the true integral value as we increase the number of points at which we sample the function. Because integration is based on randomly drawn intervals, the convergence is not smooth. If we took many integrals at each bin number, however, we would expect this plot to become smooth.\n",
        "\n",
        "![](https://github.com/dustywhite7/Econ8320/blob/master/SlidesCode/integral.png?raw=true)\n",
        "\n",
        "### Parallel calculations\n",
        "\n",
        "The ``multiprocessing`` library is designed to create multiple sub-instances of the python interpreter called processes, with each process returning values that are independent of the other processes. In order to coordinate this effort, some computational power must be assigned to send off processes and to retrieve their results upon completion.\n",
        "\n",
        "This \"overhead\" means that parallel processing is not usually justified for very simple problems, and is reserved for computationally intensive problems, or where there are very many processes to be completed in one batch.\n",
        "\n",
        "Given the overhead of parallelization, it is not worthwhile to make a parallel version of our single integral function above. Instead, it IS worthwhile to create a function that can calculate the integral many times in parallel. Like we said above, convergence is noisy, but an average value for each given number of bins should be more stable.\n",
        "\n",
        "Let's see how much time we can save by making a function that will calculate an average value in parallel when compared to a serialized alternative."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ah9ZBU2dF5n_"
      },
      "outputs": [],
      "source": [
        "import multiprocessing as mp # This module is part of the\n",
        "\t\t\t     # python standard library\n",
        "\n",
        "def serial_average(n_bins, n_reps, f, xmin, xmax):\n",
        "  attempts = [serial_integral(n_bins, f, xmin, xmax) for i in range(n_reps)]\n",
        "  return sum(attempts)/n_reps\n",
        "\n",
        "def parallel_average(processes, n_bins, n_reps, f, xmin, xmax):\n",
        "  pool = mp.Pool(processes=processes)\n",
        "  results = [pool.apply_async(serial_integral, (n_bins, f, xmin, xmax)) for i in range(n_reps)]\n",
        "  results = [p.get() for p in results]\n",
        "  return sum(results)/n_reps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mebIeivfF5oA"
      },
      "source": [
        "Now let's explore what that function does."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGm2cxBhF5oA"
      },
      "outputs": [],
      "source": [
        "def parallel_average(processes, n_bins, n_reps, f, xmin, xmax):\n",
        "  pool = mp.Pool(processes=processes)\n",
        "  ...\n",
        "  return ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lrYFiEMF5oA"
      },
      "source": [
        "The `mp.Pool` class provides the functionality to organize our processes, and to define the degree to which we want to spread our work across various **processes**. We can specify how many active processes there should be at any time with the number of processes.\n",
        "\n",
        "We need to take care to choose the right number of processes for our machine! In general, my experience has been that performance is best when you keep the number of processes at or below the number of cores available in your computer's processor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N1RXnceWF5oA"
      },
      "outputs": [],
      "source": [
        "def parallel_average(processes, n_bins, n_reps, f, xmin, xmax):\n",
        "  ...\n",
        "  results = [pool.apply_async(serial_integral, (n_bins, f, xmin, xmax)) for i in range(n_reps)]\n",
        "  ...\n",
        "  return ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrUG06T4F5oB"
      },
      "source": [
        "We next use the `apply_async` method to pass the values that we want our pooled instances to calculate. We need to provide the function to be executed, as well as the arguments for the function in each iteration with each each of the arguments an element in a tuple. In our case, we are not varying the arguments, so we provide a single tuple that never changes, but we could vary those arguments whenever necessary or desirable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WlnqrnpwF5oB"
      },
      "outputs": [],
      "source": [
        "def parallel_average(processes, n_bins, n_reps, f, xmin, xmax):\n",
        "  ...\n",
        "  results = [p.get() for p in results]\n",
        "  ...\n",
        "  return ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "picu4mXRF5oB"
      },
      "source": [
        "The next step is to use the `get()` method on each element of our returned processes. This will fetch the return statement values from each of the processes that we executed in the last line.\n",
        "\n",
        "At this point the parallel computation is complete. The remainder of the function will look and work just like the results from our `serial_average` method.\n",
        "\n",
        "\n",
        "### Timing it\n",
        "\n",
        "Next, it is time to write code that will allow us to test our parallel and serial performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SV6Ky3O3F5oB"
      },
      "outputs": [],
      "source": [
        "import timeit # library for timing execution of code\n",
        "\n",
        "benchmarks = [] # list to store our execution times\n",
        "\n",
        "benchmarks.append(timeit.Timer('serial_average(10000, 100, f, 0, 1)',\n",
        "  'from __main__ import serial_average, serial_integral, f').timeit(number=1))\n",
        "    # Note that we need to include a second line\n",
        "    # that imports our functions from __main__.\n",
        "    # This tells the timer what needs to be IN SCOPE\n",
        "\n",
        "benchmarks.append(timeit.Timer(\n",
        "  'parallel_average(2, 10000, 100, f, 0, 1)',\n",
        "  'from __main__ import parallel_average, serial_integral, f').timeit(\n",
        "    number=1))\n",
        "    # Need to include number of processes\n",
        "    # when timing the parallel implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfyQ_lRbF5oB"
      },
      "source": [
        "Amazing! When timing the output on my virtual machine (not at all a powerful computer), the parallel version runs nearly twice as fast (0.36 seconds vs 0.69 seconds). Even in a fairly trivial example, we see significant performance gains.\n",
        "\n",
        "When I ran a similar trial on my computer with 16 cores, I saw the following performance pattern:\n",
        "\n",
        "![w:750](https://github.com/dustywhite7/Econ8320/blob/master/SlidesCode/performance.png?raw=true)\n",
        "\n",
        "On 16 cores, the parallel version of this problem executes over 5x faster than the serial version! Some observations:\n",
        "\n",
        "- This was done on a 16-core processor, which is an expensive way to get a performance gain\n",
        "- Creating too many processes (going past 16 to 32) actually started to slow the computations down\n",
        "- We need to be aware of the hardware that we are utilizing when designing parallel code\n",
        "\n",
        "If you are unsure of how many processes you should run on your machine, the following command will show you how many CPU cores are available to Python:\n",
        "\n",
        "```python\n",
        "mp.cpu_count() # Tells us the number of available CPUs\n",
        "```\n",
        "\n",
        "Now it's your turn to give it a go!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbdmeG9BF5oC"
      },
      "source": [
        "## Solve-it!\n",
        "\n",
        "Simulate 1,000 draws from a multivariate normal distribution and calculate the average value of the dependent variable, but do it 100 times! This is called \"bootstrapping\" and is a critical process in many statistical models and simulations. Draw from the following equation:\n",
        "\n",
        "$$ y = \\alpha + x_{1} + 2\\cdot x_{2} + \\frac{1}{2}x_{3} + \\epsilon $$\n",
        "\n",
        "where\n",
        "\n",
        "$\\alpha \\sim \\mathcal{N}(15,2)$, $x_1 \\sim \\mathcal{N}(3,5)$,\n",
        "\n",
        "$x_2 \\sim \\mathcal{N}(10,1)$, $x_3 \\sim \\mathcal{N}(8,8)$, and\n",
        "\n",
        "$\\epsilon \\sim \\mathcal{N}(0,1)$\n",
        "\n",
        "$\\mathcal{N}(\\mu, \\sigma)$ represents the Normal distribution with mean $\\mu$ and standard deviation $\\sigma$\n",
        "\n",
        "Write functions to generate all values and calculate . Test and time these draws using serial and parallel programming (2 cores), and report the difference in performance between the two versions.\n",
        "\n",
        "\n",
        "Call your serial function `sCalc`, and your parallel function `pCalc`, store your results from each function as a list, with the lists named `sResults` and `pResults`, respectively.\n",
        "\n",
        "Finally, store the timed values for each run as `sTime` and `pTime`, respectively. All code for this exercise should go in the cell labeled `#si-exercise` below.\n",
        "\n",
        "**NOTE: Because we are running code on a virtual machine, and because the problem is not very computationally intensive, we should not be surprised if the serial version of our code outperforms the parallel version for this assignment.**\n",
        "\n",
        "**OTHER NOTE: Because we need this to run in a reasonable amount of time, we are only running 100 rounds of sampling. Typical bootstrapping procedures default to 10,000 rounds as good practice.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "n2D0WoWkF5oC"
      },
      "outputs": [],
      "source": [
        "#si-exercise\n",
        "\n",
        "import numpy as np\n",
        "import multiprocessing as mp\n",
        "import random\n",
        "import timeit\n",
        "\n",
        "sResults = []\n",
        "pResults = []\n",
        "sTime = []\n",
        "pTime = []\n",
        "\n",
        "\n",
        "def y():\n",
        "  aplha =np.random.normal(15,2)\n",
        "  x1 = np.random.normal(3,5)\n",
        "  x2 = np.random.normal(10,1)\n",
        "  x3 = np.random.normal(8,8)\n",
        "  error = np.random.normal(0,1)\n",
        "  y = aplha + x1 + 2*x2 + 0.5*x3 + error\n",
        "  return y\n",
        "\n",
        "def integral(nSample, y, xmin, xmax):\n",
        "  sample = np.sort(np.random.uniform(0,1,100)) #sorted([random.uniform(xmin, xmax) for i in range(nSample)])\n",
        "  value = [y() for _ in sample] #y(sample) #\n",
        "  area = np.sum(np.diff(sample) * value[1:])\n",
        "  #area = [(sample[0]-xmin)*value[0]] + [(sample[i]-sample[i-1])*value[i] for i in range(1, len(sample))]\n",
        "  return area #sum(area)\n",
        "\n",
        "def sCalc(n_bins, n_reps, y, xmin, xmax):\n",
        "  values = [integral(n_bins, y, xmin, xmax) for i in range(n_reps)]\n",
        "  averageS = sum(values)/n_reps\n",
        "  sResults.append(averageS)\n",
        " # sTime.append(timeit.Timer('sCalc(1000, 100, y, 0, 1)',\n",
        "  #'from __main__ import integral, sCalc, y').timeit(number=1))\n",
        "  return averageS\n",
        "\n",
        "\n",
        "def pCalc(processes, n_bins, n_reps, y, xmin, xmax):\n",
        "  pool = mp.Pool(processes=processes)\n",
        "  results = [pool.apply_async(integral, (n_bins, y, xmin, xmax)) for i in range(n_reps)]\n",
        "  results = [p.get() for p in results]\n",
        "  averageP = sum(results)/n_reps\n",
        "  pResults.append(averageP)\n",
        "  return averageP\n",
        "\n",
        "\n",
        "    #return 15 + x1 + 2*x2 + 0.5*x3 + np.random.normal(0,1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#testing code\n",
        "\n",
        "sCalc(1000, 100, y, 0, 1)\n",
        "pCalc(2, 1000, 100, y, 0, 1)\n",
        "\n",
        "print(sResults)\n",
        "print(pResults)\n",
        "print(sTime)\n",
        "print(pTime)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "o55M9K0IuN--",
        "outputId": "6304350e-560f-48f1-9c4a-fb10af678689"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[41.21371168340916]\n",
            "[41.32882389105807]\n",
            "[]\n",
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "(bool(re.search(r'def pCalc(.*\\n)+.*mp.Pool', pCalc)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "sYBdnyG90Nsr",
        "outputId": "6a2fb198-7169-4bf1-c89c-877172456f40"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "expected string or bytes-like object",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-5cfa37e08866>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'def pCalc(.*\\n)+.*mp.Pool'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpCalc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.10/re.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(pattern, string, flags)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \"\"\"Scan through string looking for a match to the pattern, returning\n\u001b[1;32m    199\u001b[0m     a Match object, or None if no match was found.\"\"\"\n\u001b[0;32m--> 200\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PvvWpecCKLlP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CGN2xZvnKLzd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LECTURE EXAMPLE CODING"
      ],
      "metadata": {
        "id": "VCHaY38IKDWF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#numeric integration (calculating the area under a curve or the area under a funcgtion )\n",
        "#1 choose points at which to estimate the value of the function\n",
        "#2 choose bandwith (based on how far apart each point is)\n",
        "#multiply function values by bandwith\n",
        "#add all estimates to calculate approx integral\n",
        "\n",
        "import numpy as np #give us the mathmatical functions\n",
        "import multiprocessing as mp #use to make our code parallel\n",
        "# define any function here!\n",
        "def f(x):\n",
        "    # return the value of the function given x\n",
        "    return 1/(1 + x**2)"
      ],
      "metadata": {
        "id": "gIZv_mFFKBeu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$ f(x) = \\frac{1}{1+x^2} $$   <-this is our function"
      ],
      "metadata": {
        "id": "uvnGCSbNPkOu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#now can use teh defined function to actually processs the function above and spit out the function at different values of x\n",
        "f(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHe47UVaPvrQ",
        "outputId": "533f3817-c51c-48ed-90ea-00ee66bcd811"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#now lets integrate this function numerically ()\n",
        "\n",
        "#nsample is the number of diff points at which we want to sample function (this lets us know how many random numbers to sample)\n",
        "\n",
        "import random\n",
        "\n",
        "def serial_integral(nSample, f, xmin, xmax):\n",
        "  # determine points of estimation\n",
        "  sample = sorted([random.uniform(xmin, xmax) for i in range(nSample)])\n",
        "  # Calculate height at each point\n",
        "  value = [f(i) for i in sample]\n",
        "  # Calculate areas of rectangles\n",
        "  # We have to specially calculate the first rectangle,\n",
        "  #   because xmin is not part of our list of samples\n",
        "  area = [(sample[0]-xmin)*value[0]] + [(sample[i]-sample[i-1])*value[i] for i in range(1, len(sample))]\n",
        "  # Return sum as an approximate integral\n",
        "  return (nSample,sum(area))\n",
        "\n",
        "\n",
        "\n",
        "#now we can test the integral function we made above\n",
        "serial_integral(1000, f, 0, 1) #increasing the number if samples gives a value closer to the truth, increasing the number samples also slows down the function though so have\n",
        "#to find a balance"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mnxAL2oQqrD",
        "outputId": "c4ac51e2-6998-4ea9-cb18-fac416ef2d0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 0.783446908884076)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#going to do what is being done in the above code block just written out oursleves\n",
        "# we want intrgrate between 0 and 1 and have 100 random points\n",
        "\n",
        "#np.sort will then put all of the points in order from lowest point to highest\n",
        "test = np.sort(np.random.uniform(0,1, 100))\n",
        "\n",
        "#next line will pass the array into the function f we defined earlier with each value of x\n",
        "f(test) #this gives us all the diff functional values across our sample fpace\n",
        "\n",
        "value = f(test)\n",
        "\n",
        "np.diff(test) #this is taking the difference between each point and the point next to it (gives us the first thing we need to caculate area of each rectangels )\n",
        "\n",
        "\n",
        " #(we'll skip over the first value in the list so that we have only 99 values)\n",
        "np.diff(test) * value[1:] #<- this the area of each rectangle rectangele\n",
        "\n",
        "#now we'll add up eahc of the values together\n",
        "np.sum(np.diff(test) * value[1:] ) #between 100 samples between 0 and 1 we estimate that the area under teh curve of this function is whatever this value is\n",
        "\n",
        "#this whole thing gives us the numeric integral of any given function (in this case its the one we defined above)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_247YjSRgr-",
        "outputId": "8575a94a-b8d3-4e3f-cef5-c21881f27ce8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7656031476057898"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#how can we use multiprocessing on this\n",
        "\n",
        "#lets calucate the avergage integral many times\n",
        "\n",
        "#well do this serially fisrs\n",
        "\n",
        "#n_bins is the sample size, n_reps is how many attempts at integrating we want to make\n",
        "\n",
        "def serial_average(n_bins, n_reps, f, xmin, xmax): #use the integral function over and over in order to get an average\n",
        "  values = [serial_integral(n_bins, f, xmin, xmax)[1] for i in range(n_reps)] #once for every value in in the range of n_reps times calculate the integral, then return the mean of this\n",
        "  return sum(values)/n_reps\n",
        "\n",
        "serial_average(200, 100, f, 0, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_lcoXkeZMT4",
        "outputId": "fe8a2e14-eaad-4c15-fd33-ab46e0d1d6f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7804129354640388"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we can write this same code above in parellel, then we can calcualte various integral simultaneously\n",
        "import multiprocessing as mp # This module is part of the\n",
        "\t\t\t     # python standard library\n",
        "\n",
        "def serial_average(n_bins, n_reps, f, xmin, xmax):\n",
        "  attempts = [serial_integral(n_bins, f, xmin, xmax)[1] for i in range(n_reps)]\n",
        "  return sum(attempts)/n_reps\n",
        "\n",
        "def parallel_average(processes, n_bins, n_reps, f, xmin, xmax):\n",
        "  pool = mp.Pool(processes=processes)\n",
        "  results = [pool.apply_async(serial_integral, (n_bins, f, xmin, xmax)) for i in range(n_reps)]\n",
        "  results = [p.get()[1] for p in results] #super important to have the [1] here, because we need the second element in our tuple of results not the first which would be at  [0]\n",
        "  return sum(results)/n_reps"
      ],
      "metadata": {
        "id": "iACTbRjjbgEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#now we will break down the parallel average process\n",
        "\n",
        "\n",
        "pool = mp.Pool(processes=processes)\n",
        "#process is where we define the number of parellel processes we'll allow (there are contrainsts on how many you want at a given time)\n",
        "#each process will be assigned to do different things\n",
        "#We also dont have to do processes as an input and can have teh computer calculate how many we need\n",
        "\n",
        "\n",
        "results = [pool.apply_async(serial_integral, (n_bins, f, xmin, xmax)) for i in range(n_reps)]\n",
        "#next with the first results = we can create a list comprehension of all of the different tasks we want the program to exicute\n",
        "#then we use teh pool object to apply these functions asynchronously (meaning they can all run at anytime and an order)\n",
        "#then whatver n_reps is is how many times we will calcualte this integral\n",
        "#the list comprehehnsion is basically like a list of to does <- this line exaplins what we want to happen but doesnt actually execute the code to do this yet\n",
        "\n",
        "results = [p.get() for p in results]\n",
        "#in results = p.get this is where the calcualtions are actually happening. This fetches the results for everything in our list. They will keep completeing asks until this is complete\n",
        "#this will create a list comprhension and then fill in each result\n",
        "\n",
        "#return sum(results)/n_reps\n",
        "#once we get the results were are looking for the sum of values and dividing it by the number of reps in order to get an average (just like in our serail average)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "collapsed": true,
        "id": "WIt7vHOsgDy1",
        "outputId": "77513365-cbd4-4570-c152-54e99d94b41d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'processes' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-a2c3ce788bca>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mpool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocesses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprocesses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m#process is where we define the number of parellel processes we'll allow (there are contrainsts on how many you want at a given time)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#each process will be assigned to do different things\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'processes' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#now we can test out or parellel average function\n",
        "\n",
        "parallel_average(4, 2000, 500, f, 0, 1)\n",
        "\n",
        "#this value should looks similar to our serical program"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Av-S3jYcidfP",
        "outputId": "b9d4e0c3-c58c-42cb-c9e5-00cfda023114"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7849030219285045"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#lets check how our parellel function performs relative to our serial function\n",
        "\n",
        "import timeit # library for timing execution of code <-checks the perforamnce of out code\n",
        "\n",
        "benchmarks = [] # create empty list to store our execution times\n",
        "\n",
        "benchmarks.append(timeit.Timer('serial_average(10000, 100, f, 0, 1)', #this ill find the time for doing our serial avaeraeg\n",
        "  'from __main__ import serial_average, serial_integral, f').timeit(number=1)) #imports functions from our own code that we coded above\n",
        "    # Note that we need to include a second line\n",
        "    # that imports our functions from __main__.\n",
        "    # This tells the timer what needs to be IN SCOPE\n",
        "\n",
        "benchmarks.append(timeit.Timer( #this will find the time of doing our pareallel average\n",
        "  'parallel_average(2, 10000, 100, f, 0, 1)',\n",
        "  'from __main__ import parallel_average, serial_integral, f').timeit(\n",
        "    number=1))\n",
        "    # Need to include number of processes\n",
        "    # when timing the parallel implementation"
      ],
      "metadata": {
        "id": "E6kRktFEjxzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#now if we looks at the benchmarks\n",
        "\n",
        "benchmarks\n",
        "\n",
        "#we can compare the 2 speeds we got from integral and parallel functions and see which ran faster"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2O1WtdFhk3Xa",
        "outputId": "5071a6f1-597f-425b-8ea5-f1934265a227"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.0172101690004638, 1.117085066999607]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#we can check the ideal number of processes to run our compuer by using\n",
        "mp.cpu_count() #this will ecentially tell us the number of avalilable cpu\n",
        "\n",
        "#since we got 2, we can increase teh processes up to 2 and expect to see perforamce imporvements. Any number after 2 we will expect to see the perforamnce decline"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyQkWRHllgwY",
        "outputId": "92195a2d-49a4-478f-9dab-922716efc776"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Create Assignment",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}